### 【线性可分的SVM原理推导】 ###

# <线性可分的定义>：
# 假设D0和D1是n为欧式空间里的两个点集。若存在n维向量w和实数b，使得所有属于D0的点xi都有w*xi + b > 0，而对于所有属于D1的点xj都有w*xj + b < 0
# 则我们称D0和D1是线性可分的。


# <目标>：寻找一个n维的超平面，使得支持向量（距离平面最近的两类数据点）间的距离最大化。
# 也可以理解为求解支持向量所在的两个平面间距margin最大化，margin=2d, d为支持向量到超平面的距离。


# 一、<SVM的最优化问题>

# 多维空间中的超平面方程: wTx + b = 0  ...(1)

# 空间中数据点到超平面的距离: d = |wTx+b| / ||w||  ...(2)

# 显然，处于超平面之上的点满足: wTx+b>0, y=1（标签为1）; 超平面之下的点满足: wTx+b<0, y=-1（标签为-1）  ...(3)

# 因为我们不允许超平面上有点存在，并尽可能大的增加这个margin=2d，因此有距离不等式: (wTx + b) / ||w|| >= d, y = 1; (wTx + b) / ||w|| <= -d, y = -1 (变形）
# 将d除到左边可得: (wTx + b) / d||w|| >= 1, y = 1; (wTx + b) / d|w|| <= -1, y = -1  ...(4)

# 由于||w||和d都是绝对正数, 我们为了方便推导（对目标函数无影响），暂令其为1, 则得: wTx + b >= 1, y = 1; wTx + b <= -1, y = -1 ...(5)

# 因此, 对于所有样本x，都要满足一个最小间距的不等式约束: yi * (wT * xi + b) > 1 （正正得正，负负得正），简记为 y(wx+b) > 1  ...(6)

# 至此，我们得到了最大间隔超平面和其上下两个支持向量平面的方程: wTx + b = 0, wTx + b = 1, wTx + b = -1  ...(7)

# 现在，两个支持向量到超平面的距离又可以写为: d = |wTx + b| // ||w||, 将(7)中平面方程带入得: d1 = |1| // ||w|| 和 d2 = |-1| // ||w||
# 则, 两个支持平面的距离为: d1 + d2 = margin = 2 // ||w||  ...(8)

# 由于svm的目标是最大化margin值，即: max{ 2 // ||w|| }, 取倒数等价于 min{ ||w|| / 2 }  ...(9)

# 为了便于计算||w||的值, 取平方数: min{ ||w||^2 / 2 }  ...(10)

# 所以最终得到的最优化问题是: min{ ||w||^2 / 2 }, s.t. yi * (wT * xi + b) >= 1  ...(11)  注: s.t.是约束条件的符号


# 二、<SVM的对偶问题>

# 拉格朗日乘数法可以解决"等式约束"的优化问题, 其形式为: min f(x1,x2,...,xn), s.t. hk(x1,x2,...,xn) = 0, k为方程的个数  ...(1)

# 我们令 L(x,λ) = f(x) + Σλk*hk(x), 函数L(x,λ)称为拉格朗日函数，参数λ称为拉格朗日乘子  ...(2)

# 利用必要条件求偏导法则，可以找到极值点的坐标:  ∂L / ∂xi = 0 (i=1,2,...,n) ; ∂L / ∂λk = 0 (k=1,2,...,l)  ...(3)

# 等式约束下的拉格朗日乘数法引入了l个乘子，我们将xi与λk一视同仁，把λk也看作优化的变量，则共有n+l个优化变量

# 对于"不等式约束"的优化问题，主要的思想是: 将不等式约束条件转变为等式约束条件，引入松弛变量，将松弛变量也视作优化变量

# 对于<一>中得出的优化函数 f(w) = min{ ||w||^2 / 2 }, s.t. yi * (wT * xi + b) >= 1 进行变形: s.t. gi(w) = 1 - yi * (wT * xi + b) <= 0  ...(4)

# 引入松弛变量ai, hi(w,ai) = gi(w) + ai = 0 （不等式加上ai后变为0），由于这样还需要要求ai>=0，为了不引入新的约束，直接将ai换成ai^2就能保证hi(w,ai)=0
# 故约束条件变为: s.t. hi(w,ai) = gi(w) + ai^2 = 0  ...(5)

# 由此我们已将"不等式约束"转化为了"等式约束"，并得到拉格朗日函数: L(w,λ,a) = f(w) + Σλi*hi(w,a) = f(w) + Σλi*(gi(w) + ai^2), λi>=0  ...(6)

# 由"等式约束"优化问题极值的必要条件对其求解，联立方程 ...(7):
# 1. ∂L / ∂wi = ∂f / ∂wi + Σλi * ∂gi / ∂wi = 0
# 2. ∂L / ∂ai = 2 * λi * ai = 0
# 3. ∂L / ∂λi = gi(w) + ai^2 = 0
# 4. λi >= 0  注: 这里要求乘子非负性

# 针对上述第2项方程, 2 * λi * ai = 0 <-> λi * ai = 0, 有以下几种情况:
# 1. 当 λi = 0, ai ≠ 0 时, 约束条件gi(w)无效, 由第3项 gi(w) + ai^2 = 0 得出 gi(w) < 0;
# 2. 当 ai = 0, λi > 0 时, 约束条件gi(w)有效, 由第3项 gi(w) + ai^2 = 0 得出 gi(w) = 0;
# 3. ai 和 λi 不能同时为零;
# 综合可得: λi * gi(w) = 0, 且当 λi > 0 时, gi(w) = 0; λi = 0, gi(w) < 0.

# 由此可将方程转换为 ...(8):
# 1. ∂L / ∂wi = ∂f / ∂wi + Σλi * ∂gi / ∂wi = 0
# 2. λi * gi(w) = 0
# 3. gi(w) <= 0
# 4. λi >= 0
# 以上便是不等式约束优化优化问题的KKT(Karush-Kuhn-Tucker) 条件，λi 称为 KKT 的乘子

# KKT条件可以理解为: 对于支持向量（不论正负）而言 gi(w) = 0, 则 λi > 0; 对于其他向量而言 gi(w) < 0, λi = 0

# 原来的目标函数 min L(w,λ,a) = f(w) + Σλi*(gi(w) + ai^2) = f(w) + Σλi*gi(w) + Σλi*ai^2
# 由于 λi*ai^2 >= 0, 则可以将问题转为: min L(w,λ) = f(w) + Σλi * gi(w)  ...(9)

# 假设现在找到了最佳参数w使得目标函数f(w)取得了最小值p, 即 ||w||^2 / 2 = p. 根据λi>=0, gi(w) <= 0 可知 Σλi * gi(w) <= 0
# 故 L(w,λ) = f(w) + Σλi * gi(w) <= f(w) = p, 即 L(w,λ) <= p. 那么为了找到最优参数λ,使得L(w,λ)接近于p，则问题转换为 max(λ){ L(w,λ) }.
# 所以现在的最优化问题为 min(w) max(λ) L(w,λ), s.t. λ >= 0  ...(10)

# 若 L(w,λ) 是一个凸函数, 且满足KKT条件时, 原问题可转换成相应的对偶问题: min(w) max(λ) L(w,λ) = max(λ) min(w) L(w,λ), s.t. λ >= 0  ...(11)
# 对于对偶问题, 一般有: min max f >= max min f , 即最大的里面挑出来的最小的也要比最小的里面挑出来的最大的要大（弱对偶关系）


# 三、<SVM的优化问题>

# 步骤1: 构造拉格朗日函数
# min(w,b) max(λ) L(w,b,λ) = f(w) + Σλi * gi(w,b) = ||w||^2 / 2 + Σλi * (1 - yi * (wT * xi + b)), s.t. λi >= 0  ...(1)

# 步骤2: 利用凸函数和KKT条件的强对偶性转换问题为: max(λ) min(w,b) L(w,b,λ), 先对内层min进行求解.
# 分别对 w 和 b 求偏导数并令为零: ∂L / ∂w = w - Σλi * yi * xi = 0, ∂L / ∂b = - Σλi * yi = 0;
# 化简得: w = Σλi * yi * xi, Σλi * yi = 0  ...(2)

# 将(2)式带入(1)式得: L(w,b,λ) = 1/2 * ΣΣλi*λj*yi*yj*xi*xj + Σλi - Σλi*yi*(Σλj*yj*xi*xj + b)
# 等价于 L(w,b,λ) = 1/2 * ΣΣλi*λj*yi*yj*xi*xj + Σλi - ΣΣλi*yi*(λj*yj*xi*xj) - Σλi*yi*b
# 由于Σλi * yi = 0, 则 Σλi*yi*b = 0, 得: L(w,b,λ) = 1/2 * ΣΣλi*λj*yi*yj*xi*xj + Σλi - ΣΣλi*yi*(λj*yj*xi*xj)
# 等价于 L(w,b,λ) = Σλi - 1/2 * ΣiΣj λi*λj*yi*yj*xiT*xj  ...(3) 注: xi * xj 相当于转置相乘
# 所以现在内层的极小值已获得: min L(w,b,λ) = Σλi - 1/2 * ΣiΣj λi*λj*yi*yj*xiT*xj, s.t.Σλi * yi = 0, λi >= 0  ...(4)

# 步骤3: 利用SMO法求解 max(λ) L(w,b,λ) 函数
# 对于该二次规划问题，我们常用 SMO(Sequential Minimal Optimization) 序列最小优化算法进行求解
# 基本思想是: 每次只优化一个参数，其他参数先固定住，仅求当前这个优化参数的极值

# 1. 选择两个需要更新的参数(某两个特征i,j) λi, λj; 固定其他参数（视作常数）, 由于Σλi*yi=0的约束，则有: Σλi*yi=0, Σλj*yj=0
# 令 C = -Σλk*yk (k≠i,j), 则约束变为: λi*yi + λj*yj = C, λi,j >= 0  ...(4)
# 于是一个变量 λj = (C - λi*yi) / yj, 即用λi来表达λj  ...(5)
# 相当于把目标问题转化成了仅有一个约束条件的最优化问题，仅有的约束是 λi >= 0;

# 2. 对于仅有一个约束条件的最优化问题，我们完全可以在 λi 上对优化目标求偏导，令导数为零，从而求出变量值 λi(new), 然后根据 λi(new) 得出 λj(new)

# 3. 多次迭代直至收敛, 最终通过 SMO 求得最优解 λ'

# 步骤4: 求取 w 和 b
# 由式(2)可知, w = Σλi * yi * xi;
# 由于对支持向量而言gi(w)=0, λi>0, 代入任意一个支持向量点得: ys * (wTxs + b) = 1 可求出b:
# 两侧同时乘以ys: ys^2 * (wTxs + b) = ys, 而 ys^2 = 1 --> b = ys - wTxs  ...(6)
# 为了增加对离群点的鲁棒性, 我们可以求得支持向量的均值（设有S个支持向量点）: b = Σbs / S  ...(7)

# 步骤6: 根据 w 和 b 求解最大间隔的超平面
# 超平面公式: wTx + b = 0
# 分类决策函数 sign(x) = 1 if x > 0, else 0 if x = 0, else -1 if x < 0, 将新样本点导入决策函数中即可进行分类
