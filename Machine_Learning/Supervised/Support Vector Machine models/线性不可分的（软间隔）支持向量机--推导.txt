### 【线性不可分的SVM原理推导】 ###

# <线性不可分的解法>:
# 将低维不可分样本映射到高维空间中，让样本点在高维空间线性可分
# 即, 在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM
# 我们用 x 表示原来的样本点，用φ(x)表示 x 映射到特征新的特征空间后到新向量。那么分割超平面可以表示为: f(x) = wTφ(x) + b  ...(1)
# 对于非线性（不可分）的SVM，对偶问题就变成了: max(λ) L(w,b,ξ,λ,μ) = Σλi - ΣΣλi*λj*yi*yj*(φ(xi)T*φ(xj)) / 2  ...(2)
# 约束条件: Σλi * yi = 0,  λi >=0, C - λi - μi = 0
# 可以看到与线性 SVM 唯一的不同就是：之前的 (xiT*xj) 变成了 (φ(xi)T*φ(xj))

# <核函数的意义>:Kernel
# 引入低维->高维的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数
# 核函数定义为: K(x,y)=<ϕ(x),ϕ(y)>, 即原始样本x进行映射后ϕ(x)的内积<ϕ(x),ϕ(y)>叫做核函数.
# 一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低
# 所以,核函数和映射本身没有关系。核函数只是用来计算映射到高维空间之后的内积的一种简便方法
# 通俗理解:
# 1. 在SVM不论是硬间隔还是软间隔在计算过程中，都有X转置点积X，若X的维度低一点还好算，但当我们想把X从低维映射到高维的时候（让数据变得线性可分时），这一步计算很困难
# 2. 也就是在计算时，需要先计算把X映射到高维的的ϕ(x)，再计算ϕ(x1)和ϕ(x2)的点积，这一步计算起来开销很大，难度也很大，此时引入核函数，这两步的计算便成了一步计算
# 3. 即只需把两个x带入核函数，计算核函数即可
# 举例说明: 假设我们有一个多项式核函数 K(x,y) = (x * y + 1)^2, 带入样本点后 K(x,y) = (Σ(xi * yi) + 1)^2
# 其展开项是: Σ(xi^2 * yi^2) + ΣΣ(√2*xi*xj)(√2*yi*yj) + Σn*(√2*xi)(√2*yi) + 1
# 如果没有核函数，我们则需要把向量映射成: x‘ = (x1^2,x2^2,...,xn^2,...,√2*x1,√2*x2,...,√2*xn,,1)
# 然后再进行X的内积计算，才能与多项式核函数达到相同的效果.
# 意义:
# 核函数的引入一方面减少了我们计算量，另一方面也减少了我们存储数据的内存使用量
# 它接收两个变量，这两个变量是在低维空间中的变量，而核函数求的值等于将两个低维空间中的向量映射到高维空间后的内积.

# <如何选择核函数>:
# 1. 当特征维数 d 超过样本数 m 时 (文本分类问题通常是这种情况), 使用线性核;
# 2. 当特征维数 d 比较小. 样本数 m 中等时, 使用RBF核;
# 3. 当特征维数 d 比较小. 样本数 m 特别大时, 支持向量机性能通常不如深度神经网络
# 4. 首选高斯(RBF)核, 仅RBF核需要调参

# <核函数的表达形式>:
# 1. 线性核: K(xi,xj) = xTi*xj
# 2. 多项式核: K(xi,xj) = (γ*(xTi*xj) + c)^d
# 3. 高斯核: K(xi,xj) = exp(-||xi - xj|| / 2δ^2)
# 4. Sigmoid核: K(xi,xj) = tanh(γ*(xTi*xj) + c)

# <SVM的优缺点>:
# 优点：
# 由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。
# 不仅适用于线性线性问题还适用于非线性问题(用核技巧)。
# 拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。
# 理论基础比较完善, 逻辑优美。

# 缺点：
# 二次规划问题求解将涉及m阶矩阵的计算(m为样本的个数), 因此SVM不适用于超大数据集。(SMO算法可以缓解这个问题)
# 只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)